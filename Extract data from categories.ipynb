{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This code extracts documents and related comments from documents in different categories. There are 10 categories in regulation.gov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pickle import dump, load\n",
    "import pandas\n",
    "import requests\n",
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "import PyPDF2\n",
    "api_key = 'vT4R3vZ8RpZhnCpgeCPx1LdWRSZS8yxHHGquPrxm'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# def read_file_get_docid(filepath):\n",
    "#     dump_df = load(open(filepath,'rb'))\n",
    "#     df_with_comments = dump_df[dump_df.numberOfCommentsReceived > 0]\n",
    "#     df_with_comments =df_with_comments.sort(['numberOfCommentsReceived'], ascending=[False])\n",
    "#     doc_id = df_with_comments.documentId\n",
    "#     doc_type = df_with_comments.documentType\n",
    "#     #return [doc_id,set(doc_type)]\n",
    "#     return df_with_comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Proposed Rule', 'Rule', 'Other', 'Notice'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kinshuk/anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:4: FutureWarning: sort(columns=....) is deprecated, use sort_values(by=.....)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4421"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# [doc_id_list,types] = read_file_get_docid('data/AD_doc_list') #choose the category dump file\n",
    "# print(types)\n",
    "# # document ID with 4 parts represent documents. 3 parts represent dockets \n",
    "# doc_ids = [doc_id for doc_id in doc_id_list if len(doc_id.split('-')) == 4]\n",
    "# len(doc_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using regulations.gov API\n",
    "* We need to use the API to retrieve each document content. This API will use document_id that we extracted from the file above.\n",
    "* For each document_id, we will need to construct comment_id based on the total number of comments on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def download_file(download_url):\n",
    "    try:\n",
    "        response = urllib.request.urlopen(download_url)\n",
    "        file = open(\"document.pdf\", 'wb')\n",
    "        file.write(response.read())\n",
    "        file.close()\n",
    "    except:\n",
    "        print(\"(downloading the pdf exception)error log\" +  download_url)\n",
    "    \n",
    "def get_attached_comments(comment_id, key=api_key):\n",
    "    #print(each_id) # fro debugging\n",
    "    #open the api to get file url\n",
    "    url = \"http://api.data.gov:80/regulations/v3/document.json?api_key=\"+key+\"&documentId=\"+comment_id\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "    except:\n",
    "        print(\"(api opening of attached comment exception)error log\" +  url)\n",
    "    if response.status_code != 200:\n",
    "        print(\"status code \" +str(response.status_code)+\" (get_attached_comments) program will break at this point which is ok because we dont need inconsistent data. Run again \")\n",
    "    data = response.json()\n",
    "    if \"fileFormats\" in data[\"attachments\"][0]:\n",
    "        att_count = len(data[\"attachments\"][0][\"fileFormats\"])\n",
    "    else:\n",
    "        att_count = len(data[\"attachments\"][1][\"fileFormats\"])\n",
    "    comment_text =\"\"\n",
    "    for i in range(att_count):\n",
    "        if data[\"attachments\"][0][\"fileFormats\"][i].endswith(\"pdf\"):\n",
    "            link = data[\"attachments\"][0][\"fileFormats\"][i] \n",
    "            access_link = link+'&api_key='+key\n",
    "            #download file(pdf) and read pdf (page by page)\n",
    "            download_file(access_link)\n",
    "            pdfFileObj = open('document.pdf','rb')     #'rb' for read binary mode\n",
    "            try:\n",
    "                pdfReader = PyPDF2.PdfFileReader(pdfFileObj)\n",
    "                pno = pdfReader.numPages\n",
    "                for i in range(pno):\n",
    "                    pageObj = pdfReader.getPage(i)          #'i' is the page number\n",
    "                    comment_text += pageObj.extractText()\n",
    "            except:\n",
    "                print(\"(pdf exception)cant read \"+comment_id ) # prints in case we are not able to read file\n",
    "            break # execute the whole thing for 1st found pdf\n",
    "    return comment_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_document_comments_from_api(docketId,key=api_key):\n",
    "    offset=0\n",
    "    url = \"http://api.data.gov:80/regulations/v3/documents.json?api_key=\"+key+\"&countsOnly=1&dct=PS&dktid=\"+docketId\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "    except:\n",
    "        print(\"(api opening comment count)error log\"+url) # prints in case we are not able to read file\n",
    "    if response.status_code != 200:\n",
    "        print(\"status code\"+str(response.status_code) + \" (get_document_comments_from_api) program will break at this point which is ok because we dont need inconsistent data. Run again\")\n",
    "    data = response.json()\n",
    "    total = data['totalNumRecords']\n",
    "    com_list =[]\n",
    "    for i in range(0,total,1000):\n",
    "        url = \"http://api.data.gov:80/regulations/v3/documents.json?api_key=\"+key+\"&countsOnly=0&&rpp=1000&po=\"+str(i)+\"&dct=PS&dktid=\"+docketId\n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "        except:\n",
    "            print(\"(api opening actual comments)error log\"+url) # prints in case we are not able to read file\n",
    "        #print(\"Offset:\"+str(i)+\" Code:\"+str(response.status_code))\n",
    "        if response.status_code != 200:\n",
    "            print(response.status_code)\n",
    "        data = response.json()\n",
    "        com_list += data['documents']\n",
    "    com_df = pandas.DataFrame(com_list)\n",
    "    return com_df\n",
    "\n",
    "def get_document_content_from_api(docId,key=api_key):\n",
    "    url = \"http://api.data.gov:80/regulations/v3/document.json?api_key=\"+key+\"&documentId=\"+docId\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "    except:\n",
    "         print(\"(api opening doc exception) error log\"+url)   \n",
    "    if response.status_code != 200:\n",
    "        print(\"status code \"+str(response.status_code)+\" (get_document_content_from_api) program will break at this point which is ok because we dont need inconsistent data. Run again\")\n",
    "    data = response.json()\n",
    "    \n",
    "    # Get HTML for document content\n",
    "    if(len(data['fileFormats']) == 2):    \n",
    "        link = data['fileFormats'][1] # The second link is the document in HTML format\n",
    "    else:\n",
    "        link = data['fileFormats'][0]\n",
    "    access_link = link+'&api_key='+key\n",
    "    \n",
    "    try:\n",
    "        with urllib.request.urlopen(access_link) as response:\n",
    "            html = response.read()\n",
    "    except:\n",
    "        print(\"doc file opening exception\")\n",
    "    \n",
    "    # We are interested in the pre tag of the HTML content\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "    content = soup.find_all('pre')\n",
    "    \n",
    "    # Now we need to construct comment_id from document_id\n",
    "    docket_id = '-'.join(docId.split('-')[:3])\n",
    "    comment_df = get_document_comments_from_api(docket_id)\n",
    "    # get comment text where exists\n",
    "    comment_list =[]\n",
    "    if not comment_df.empty:\n",
    "        if \"commentText\" in comment_df:\n",
    "            comment_text =comment_df[comment_df.commentText.notnull()].commentText\n",
    "            comment_list =comment_text.tolist()\n",
    "        #get doc id where there is attchment\n",
    "        c_ids = comment_df[comment_df.attachmentCount>0].documentId\n",
    "        # get comment for each id in list\n",
    "        for each_id in c_ids.unique():\n",
    "            comment_list.append(get_attached_comments(each_id))\n",
    "    doc_dict = {\n",
    "        \"text\":content,\n",
    "        \"comment_list\":comment_list\n",
    "    }\n",
    "    return doc_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Running it on one document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# data format is array of dicts\n",
    "def get_one_doc(name,docid):\n",
    "    #open file - get present data (empty to begin witj)\n",
    "    filepath = \"data/\"+name+\"_doc_content\"\n",
    "    inp =open(filepath,'rb')\n",
    "    doc_collection = load(inp)\n",
    "    inp.close()\n",
    "    #get that one doc\n",
    "    resp = get_document_content_from_api(docid)\n",
    "    doc_collection.append(resp)\n",
    "    #put back in file \n",
    "    output = open(filepath, 'wb')\n",
    "    dump(doc_collection, output, -1)\n",
    "    output.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PdfReadWarning: Xref table not zero-indexed. ID numbers for objects will be corrected. [pdf.py:1736]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(pdf exception)cant read ATBCB-2010-0001-0090\n"
     ]
    }
   ],
   "source": [
    "# run this line once every 1 hr\n",
    "get_one_doc(\"Kinshuk\",\"ATBCB-2010-0001-0001\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# BELOW THIS IS TESTING, NOT NEEDED TO GET DOCUMENT AND COMMENTS CURRENTLY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FutureWarning: sort(columns=....) is deprecated, use sort_values(by=.....) [__main__.py:4]\n"
     ]
    }
   ],
   "source": [
    "df_test =read_file_get_docid('data/PRE_doc_list')\n",
    "df_test.reset_index(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20    4630\n",
       "21    3136\n",
       "22    3123\n",
       "23    3032\n",
       "24    2441\n",
       "25    2188\n",
       "26    2076\n",
       "27    1870\n",
       "28    1684\n",
       "29    1633\n",
       "30    1488\n",
       "31    1412\n",
       "32    1333\n",
       "33    1328\n",
       "34    1228\n",
       "35    1100\n",
       "36    1085\n",
       "37    1071\n",
       "38    1025\n",
       "39     853\n",
       "40     824\n",
       "41     736\n",
       "42     713\n",
       "43     690\n",
       "44     668\n",
       "45     649\n",
       "46     575\n",
       "47     554\n",
       "48     536\n",
       "49     531\n",
       "50     524\n",
       "51     516\n",
       "52     492\n",
       "53     465\n",
       "54     450\n",
       "55     448\n",
       "56     435\n",
       "57     433\n",
       "58     400\n",
       "59     399\n",
       "60     395\n",
       "61     378\n",
       "62     377\n",
       "63     370\n",
       "64     366\n",
       "Name: numberOfCommentsReceived, dtype: int64"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test[\"numberOfCommentsReceived\"][20:65] #60,61,62,63"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60       BOR-2008-0004-0001\n",
       "61       HUD-2011-0014-0001\n",
       "62       HUD-2006-0204-0001\n",
       "63    HHS-OS-2015-0005-0001\n",
       "64       BIA-2013-0007-0087\n",
       "Name: documentId, dtype: object"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test[\"documentId\"][60:65]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
