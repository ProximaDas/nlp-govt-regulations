{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pickle import dump, load\n",
    "import nltk\n",
    "from nltk import word_tokenize,FreqDist\n",
    "import re\n",
    "from nltk.corpus import wordnet as wn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_document_text(raw_text):\n",
    "    \"\"\" This function takes in raw document text as input which we receive from the API and returns a clean text \n",
    "    of the associated document. It cleans up any HTML code in the text, newline characters, and extracts supplemental\n",
    "    information part of the document.\n",
    "    \n",
    "    INPUT: string\n",
    "    OUTPUT: string\n",
    "    \"\"\"\n",
    "    raw_text = raw_text.replace('\\n',' ')\n",
    "    raw_text = raw_text.replace('*','') # added\n",
    "    raw_text = raw_text.replace('\\r',' ') # added\n",
    "    # Remove any residual HTML tags in text\n",
    "    cleanr = re.compile('<.*?>')\n",
    "    cleantext = re.sub(cleanr, '', raw_text)\n",
    "    return cleantext\n",
    "\n",
    "def tokenize_text(corpus):\n",
    "    pattern = r'''(?x)    # set flag to allow verbose regexps\n",
    "    (([A-Z]\\.)+)       # abbreviations, e.g. B.C.\n",
    "    |(\\w+([-']\\w+)*)       # words with optional internal hyphens e.g. after-ages or author's\n",
    "    '''\n",
    "    tokens = nltk.regexp_tokenize(corpus,pattern)\n",
    "    all_token = [word.lower() for token in tokens for word in token if word != \"\" and word[0] != \"'\" and word[0] != \"-\"]\n",
    "    return all_token\n",
    "\n",
    "def tokenize_text_sent(corpus):\n",
    "    sent_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "    raw_sents = sent_tokenizer.tokenize(corpus) # Split text into sentences    \n",
    "    return [tokenize_text(sent) for sent in raw_sents]\n",
    "\n",
    "def tag_my_text(sents):\n",
    "    return [nltk.pos_tag(sent) for sent in sents]\n",
    "\n",
    "#Chunk noun phrases in tree \n",
    "def noun_phrase_chunker():\n",
    "    grammar = r\"\"\"\n",
    "    NP: {<DT|PP\\$>?<JJ>*<NN>}   # chunk determiner/possessive, adjectives and noun\n",
    "    \"\"\"\n",
    "    cp = nltk.RegexpParser(grammar)\n",
    "    return cp\n",
    "\n",
    "#Extract only the NP marked phrases from the parse tree, that is the chunk we defined\n",
    "def noun_phrase_extractor(sentences, chunker):\n",
    "    res = []\n",
    "    for sent in sentences:\n",
    "        tree = chunker.parse(sent)\n",
    "        for subtree in tree.subtrees():\n",
    "            if subtree.label() == 'NP' : \n",
    "                res.append(subtree[0:len(subtree)])\n",
    "                #res.append(subtree[0])\n",
    "                #print(subtree)\n",
    "    return res\n",
    "\n",
    "#remove tags and get only the noun phrases , can be adjusted for length\n",
    "def noun_phrase_finder(tagged_text):\n",
    "    all_proper_noun = noun_phrase_extractor(tagged_text,noun_phrase_chunker()) \n",
    "    #does not literally mean proper noun. Chunker only extracts common noun\n",
    "    noun_phrase_list = []                                                      \n",
    "    #noun_phrase_string_list =[]\n",
    "    for noun_phrase in all_proper_noun:\n",
    "        if len(noun_phrase) > 0: #this means where the size of the phrase is greater than 1\n",
    "            small_list =[]\n",
    "            for (word,tag) in noun_phrase:\n",
    "                small_list.append(word)\n",
    "            noun_phrase_list.append(small_list)\n",
    "            #noun_phrase_string_list.append(' '.join(small_list))\n",
    "    return noun_phrase_list\n",
    "\n",
    "#get freq dist obj for noun phrase of different lengths\n",
    "def find_freq(nested_list,nest_len):\n",
    "    #from nltk.probability import FreqDist\n",
    "    fdist_list =[]\n",
    "    for inner_np in nested_list:\n",
    "        if len(inner_np) == nest_len:\n",
    "            fdist_list.append(' '.join(inner_np))\n",
    "    fdist = FreqDist(fdist_list)\n",
    "    return fdist\n",
    "\n",
    "# #make a grand list of top occuring noun phrases of different sizes --- For testing purpose only. Wont be used\n",
    "# def get_top_np(np):\n",
    "#     master_common_list=[]\n",
    "#     len_list =get_length_np(np).keys()\n",
    "#     for item in len_list:\n",
    "#         fdist_np = find_freq(np_list,item)\n",
    "#         master_common_list.append(fdist_np.most_common(15))\n",
    "#     return master_common_list\n",
    "\n",
    "def get_top_unigrams(np):\n",
    "    unigrams = []\n",
    "    for item in np:\n",
    "        if len(item) ==  1:\n",
    "            unigrams.append(item)\n",
    "    fdist_uni = find_freq(np,1)\n",
    "    uni_list = fdist_uni.most_common()\n",
    "    threshold = 0.3 * len(unigrams)\n",
    "    top = []\n",
    "    s = 0\n",
    "    for word,count in uni_list:\n",
    "        top.append(word)\n",
    "        s += count\n",
    "        if s > threshold:\n",
    "            break      \n",
    "    return top\n",
    "\n",
    "# Lesk algorith for disambiguation in case of multiple synsets of a word\n",
    "def compare_overlaps_greedy(context, synsets_signatures, pos=None):\n",
    "    \"\"\"\n",
    "    Calculate overlaps between the context sentence and the synset_signature\n",
    "    and returns the synset with the highest overlap.\n",
    "    \n",
    "    :param context: ``context_sentence`` The context sentence where the ambiguous word occurs.\n",
    "    :param synsets_signatures: ``dictionary`` A list of words that 'signifies' the ambiguous word.\n",
    "    :param pos: ``pos`` A specified Part-of-Speech (POS).\n",
    "    :return: ``lesk_sense`` The Synset() object with the highest signature overlaps.\n",
    "    \"\"\"\n",
    "    # if this returns none that means that there is no overlap\n",
    "    max_overlaps = 0\n",
    "    lesk_sense = None\n",
    "    for ss in synsets_signatures:\n",
    "        if pos and str(ss.pos()) != pos: # Skips different POS.\n",
    "            continue\n",
    "        overlaps = set(synsets_signatures[ss]).intersection(context)\n",
    "        if len(overlaps) > max_overlaps:\n",
    "            lesk_sense = ss\n",
    "            max_overlaps = len(overlaps)  \n",
    "    return lesk_sense\n",
    "\n",
    "def lesk(context_sentence, ambiguous_word, pos=None, dictionary=None):\n",
    "    \"\"\"\n",
    "    This function is the implementation of the original Lesk algorithm (1986).\n",
    "    It requires a dictionary which contains the definition of the different\n",
    "    sense of each word. See http://goo.gl/8TB15w\n",
    "\n",
    "        >>> from nltk import word_tokenize\n",
    "        >>> sent = word_tokenize(\"I went to the bank to deposit money.\")\n",
    "        >>> word = \"bank\"\n",
    "        >>> pos = \"n\"\n",
    "        >>> lesk(sent, word, pos)\n",
    "        Synset('bank.n.07')\n",
    "    \n",
    "    :param context_sentence: The context sentence where the ambiguous word occurs.\n",
    "    :param ambiguous_word: The ambiguous word that requires WSD.\n",
    "    :param pos: A specified Part-of-Speech (POS).\n",
    "    :param dictionary: A list of words that 'signifies' the ambiguous word.\n",
    "    :return: ``lesk_sense`` The Synset() object with the highest signature overlaps.\n",
    "    \"\"\"\n",
    "    if not dictionary:\n",
    "        dictionary = {}\n",
    "        for ss in wn.synsets(ambiguous_word):\n",
    "            dictionary[ss] = ss.definition().split()\n",
    "    best_sense = compare_overlaps_greedy(context_sentence, dictionary, pos)\n",
    "    return best_sense\n",
    "    #return dictionary \n",
    "\n",
    "# this function takes in a word and gets the most relevant synset based on context from the text. \n",
    "# for exact algorith refer the text above (\"what I want to do\" markdown)\n",
    "def get_synset(word,pos_tag_text ,pos):\n",
    "    if len(wn.synsets(word)) == 1:\n",
    "        #print(\"here1\")\n",
    "        return wn.synsets(word)[0]\n",
    "    else:\n",
    "        #get all context sentences\n",
    "        all_sent =[]\n",
    "        for sent in pos_tag_text:\n",
    "            for (w,t) in sent:\n",
    "                if w == word:\n",
    "                    all_sent.append(sent)\n",
    "        #call lesk here\n",
    "        app_syn = lesk(all_sent[len(all_sent)//2], word, pos)\n",
    "        if app_syn != None:\n",
    "            #print(\"here2\")\n",
    "            return app_syn\n",
    "        else:\n",
    "            #second lesk trial with another context sentence\n",
    "            app_syn = lesk(all_sent[len(all_sent)//3], word, pos)\n",
    "            if app_syn != None:\n",
    "                #print(\"here2\")\n",
    "                return app_syn\n",
    "            else:\n",
    "                #give up and choose 1st synset from list with matching pos\n",
    "                #print(\"here3\")\n",
    "                all_syns = wn.synsets(word)\n",
    "                for syn in all_syns:\n",
    "                    #print(syn.pos())\n",
    "                    if syn.pos() == pos:\n",
    "                        return syn\n",
    "    return False\n",
    "\n",
    "# this functions take all the single and double legth phrases form grand_list and gets sysnset for all them. (1 each)\n",
    "def get_singles_synset(uni_list,pos_tag_text):\n",
    "    single_synset =[]\n",
    "    #get synsets of all singletons\n",
    "    for singles in uni_list:\n",
    "        singles_syn = get_synset(singles,pos_tag_text, 'n')\n",
    "        if singles_syn:\n",
    "            single_synset.append(singles_syn)    \n",
    "    return single_synset\n",
    "\n",
    "#get common parents\n",
    "def get_lcs(uni_list,pos_tag_text):\n",
    "    #get all relevant sysnsets\n",
    "    all_synsets = get_singles_synset(uni_list,pos_tag_text)\n",
    "    list_of_all_lcs =[]\n",
    "    for syn in all_synsets:\n",
    "        for syn2 in all_synsets[all_synsets.index(syn)+1:]:\n",
    "            lcs = syn.lowest_common_hypernyms(syn2)[0]\n",
    "            if lcs not in list_of_all_lcs:\n",
    "                list_of_all_lcs.append(lcs)\n",
    "    return list_of_all_lcs\n",
    "\n",
    "# get themes\n",
    "def get_theme(uni_list,pos_tag_text):\n",
    "    # get common parent\n",
    "    parent_sysnset = get_lcs(uni_list,pos_tag_text)\n",
    "    # filter out absolute top level and get lemma_names\n",
    "    lemma_names =[]\n",
    "    for synset in parent_sysnset:\n",
    "        if synset.min_depth() != 0:\n",
    "            #print(synset)\n",
    "            for each_name in synset.lemma_names():\n",
    "                if each_name not in lemma_names:\n",
    "                    lemma_names.append(each_name)\n",
    "                break\n",
    "    return lemma_names\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_cluster_count(text):\n",
    "    cleantext = get_document_text(text)\n",
    "    tagged_tokens = tag_my_text(tokenize_text_sent(cleantext))\n",
    "    np_list = noun_phrase_finder(tagged_tokens)\n",
    "    top_np = get_top_unigrams(np_list)\n",
    "    themes = get_theme(top_np,tagged_tokens)\n",
    "    return len(themes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Test\n",
    "doc_list =load(open(\"data/Master_doc_content\",'rb'))\n",
    "document = doc_list[6]\n",
    "document_text = str(document['text'][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_cluster_count(document_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
