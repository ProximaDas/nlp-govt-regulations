{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "from pickle import dump, load\n",
    "import nltk\n",
    "from nltk import word_tokenize,FreqDist\n",
    "import re\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.util import ngrams\n",
    "from sklearn.cluster import KMeans\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import paired_distances\n",
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#cluster count\n",
    "def get_document_text(raw_text):\n",
    "    \"\"\" This function takes in raw document text as input which we receive from the API and returns a clean text \n",
    "    of the associated document. It cleans up any HTML code in the text, newline characters, and extracts supplemental\n",
    "    information part of the document.\n",
    "    \n",
    "    INPUT: string\n",
    "    OUTPUT: string\n",
    "    \"\"\"\n",
    "    raw_text = raw_text.replace('\\n',' ')\n",
    "    raw_text = raw_text.replace('*','') # added\n",
    "    raw_text = raw_text.replace('\\r',' ') # added\n",
    "    # Remove any residual HTML tags in text\n",
    "    cleanr = re.compile('<.*?>')\n",
    "    cleantext = re.sub(cleanr, '', raw_text)\n",
    "    return cleantext\n",
    "\n",
    "def tokenize_text(corpus):\n",
    "    pattern = r'''(?x)    # set flag to allow verbose regexps\n",
    "    (([A-Z]\\.)+)       # abbreviations, e.g. B.C.\n",
    "    |(\\w+([-']\\w+)*)       # words with optional internal hyphens e.g. after-ages or author's\n",
    "    '''\n",
    "    tokens = nltk.regexp_tokenize(corpus,pattern)\n",
    "    all_token = [word.lower() for token in tokens for word in token if word != \"\" and word[0] != \"'\" and word[0] != \"-\"]\n",
    "    return all_token\n",
    "\n",
    "def tokenize_text_sent(corpus):\n",
    "    sent_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "    raw_sents = sent_tokenizer.tokenize(corpus) # Split text into sentences    \n",
    "    return [tokenize_text(sent) for sent in raw_sents]\n",
    "\n",
    "def tag_my_text(sents):\n",
    "    return [nltk.pos_tag(sent) for sent in sents]\n",
    "\n",
    "#Chunk noun phrases in tree \n",
    "def noun_phrase_chunker():\n",
    "    grammar = r\"\"\"\n",
    "    NP: {<DT|PP\\$>?<JJ>*<NN>}   # chunk determiner/possessive, adjectives and noun\n",
    "    \"\"\"\n",
    "    cp = nltk.RegexpParser(grammar)\n",
    "    return cp\n",
    "\n",
    "#Extract only the NP marked phrases from the parse tree, that is the chunk we defined\n",
    "def noun_phrase_extractor(sentences, chunker):\n",
    "    res = []\n",
    "    for sent in sentences:\n",
    "        tree = chunker.parse(sent)\n",
    "        for subtree in tree.subtrees():\n",
    "            if subtree.label() == 'NP' : \n",
    "                res.append(subtree[0:len(subtree)])\n",
    "                #res.append(subtree[0])\n",
    "                #print(subtree)\n",
    "    return res\n",
    "\n",
    "#remove tags and get only the noun phrases , can be adjusted for length\n",
    "def noun_phrase_finder(tagged_text):\n",
    "    all_proper_noun = noun_phrase_extractor(tagged_text,noun_phrase_chunker()) \n",
    "    #does not literally mean proper noun. Chunker only extracts common noun\n",
    "    noun_phrase_list = []                                                      \n",
    "    #noun_phrase_string_list =[]\n",
    "    for noun_phrase in all_proper_noun:\n",
    "        if len(noun_phrase) > 0: #this means where the size of the phrase is greater than 1\n",
    "            small_list =[]\n",
    "            for (word,tag) in noun_phrase:\n",
    "                small_list.append(word)\n",
    "            noun_phrase_list.append(small_list)\n",
    "            #noun_phrase_string_list.append(' '.join(small_list))\n",
    "    return noun_phrase_list\n",
    "\n",
    "#get freq dist obj for noun phrase of different lengths\n",
    "def find_freq(nested_list,nest_len):\n",
    "    #from nltk.probability import FreqDist\n",
    "    fdist_list =[]\n",
    "    for inner_np in nested_list:\n",
    "        if len(inner_np) == nest_len:\n",
    "            fdist_list.append(' '.join(inner_np))\n",
    "    fdist = FreqDist(fdist_list)\n",
    "    return fdist\n",
    "\n",
    "def get_top_unigrams(np):\n",
    "    unigrams = []\n",
    "    for item in np:\n",
    "        if len(item) ==  1:\n",
    "            unigrams.append(item)\n",
    "    fdist_uni = find_freq(np,1)\n",
    "    uni_list = fdist_uni.most_common()\n",
    "    threshold = 0.3 * len(unigrams)\n",
    "    top = []\n",
    "    s = 0\n",
    "    for word,count in uni_list:\n",
    "        top.append(word)\n",
    "        s += count\n",
    "        if s > threshold:\n",
    "            break      \n",
    "    return top\n",
    "\n",
    "# Lesk algorith for disambiguation in case of multiple synsets of a word\n",
    "def compare_overlaps_greedy(context, synsets_signatures, pos=None):\n",
    "    \"\"\"\n",
    "    Calculate overlaps between the context sentence and the synset_signature\n",
    "    and returns the synset with the highest overlap.\n",
    "    \n",
    "    :param context: ``context_sentence`` The context sentence where the ambiguous word occurs.\n",
    "    :param synsets_signatures: ``dictionary`` A list of words that 'signifies' the ambiguous word.\n",
    "    :param pos: ``pos`` A specified Part-of-Speech (POS).\n",
    "    :return: ``lesk_sense`` The Synset() object with the highest signature overlaps.\n",
    "    \"\"\"\n",
    "    # if this returns none that means that there is no overlap\n",
    "    max_overlaps = 0\n",
    "    lesk_sense = None\n",
    "    for ss in synsets_signatures:\n",
    "        if pos and str(ss.pos()) != pos: # Skips different POS.\n",
    "            continue\n",
    "        overlaps = set(synsets_signatures[ss]).intersection(context)\n",
    "        if len(overlaps) > max_overlaps:\n",
    "            lesk_sense = ss\n",
    "            max_overlaps = len(overlaps)  \n",
    "    return lesk_sense\n",
    "\n",
    "def lesk(context_sentence, ambiguous_word, pos=None, dictionary=None):\n",
    "    \"\"\"\n",
    "    This function is the implementation of the original Lesk algorithm (1986).\n",
    "    It requires a dictionary which contains the definition of the different\n",
    "    sense of each word. See http://goo.gl/8TB15w\n",
    "\n",
    "        >>> from nltk import word_tokenize\n",
    "        >>> sent = word_tokenize(\"I went to the bank to deposit money.\")\n",
    "        >>> word = \"bank\"\n",
    "        >>> pos = \"n\"\n",
    "        >>> lesk(sent, word, pos)\n",
    "        Synset('bank.n.07')\n",
    "    \n",
    "    :param context_sentence: The context sentence where the ambiguous word occurs.\n",
    "    :param ambiguous_word: The ambiguous word that requires WSD.\n",
    "    :param pos: A specified Part-of-Speech (POS).\n",
    "    :param dictionary: A list of words that 'signifies' the ambiguous word.\n",
    "    :return: ``lesk_sense`` The Synset() object with the highest signature overlaps.\n",
    "    \"\"\"\n",
    "    if not dictionary:\n",
    "        dictionary = {}\n",
    "        for ss in wn.synsets(ambiguous_word):\n",
    "            dictionary[ss] = ss.definition().split()\n",
    "    best_sense = compare_overlaps_greedy(context_sentence, dictionary, pos)\n",
    "    return best_sense\n",
    "    #return dictionary \n",
    "\n",
    "# this function takes in a word and gets the most relevant synset based on context from the text. \n",
    "# for exact algorith refer the text above (\"what I want to do\" markdown)\n",
    "def get_synset(word,pos_tag_text ,pos):\n",
    "    if len(wn.synsets(word)) == 1:\n",
    "        #print(\"here1\")\n",
    "        return wn.synsets(word)[0]\n",
    "    else:\n",
    "        #get all context sentences\n",
    "        all_sent =[]\n",
    "        for sent in pos_tag_text:\n",
    "            for (w,t) in sent:\n",
    "                if w == word:\n",
    "                    all_sent.append(sent)\n",
    "        #call lesk here\n",
    "        app_syn = lesk(all_sent[len(all_sent)//2], word, pos)\n",
    "        if app_syn != None:\n",
    "            #print(\"here2\")\n",
    "            return app_syn\n",
    "        else:\n",
    "            #second lesk trial with another context sentence\n",
    "            app_syn = lesk(all_sent[len(all_sent)//3], word, pos)\n",
    "            if app_syn != None:\n",
    "                #print(\"here2\")\n",
    "                return app_syn\n",
    "            else:\n",
    "                #give up and choose 1st synset from list with matching pos\n",
    "                #print(\"here3\")\n",
    "                all_syns = wn.synsets(word)\n",
    "                for syn in all_syns:\n",
    "                    #print(syn.pos())\n",
    "                    if syn.pos() == pos:\n",
    "                        return syn\n",
    "    return False\n",
    "\n",
    "# this functions take all the single and double legth phrases form grand_list and gets sysnset for all them. (1 each)\n",
    "def get_singles_synset(uni_list,pos_tag_text):\n",
    "    single_synset =[]\n",
    "    #get synsets of all singletons\n",
    "    for singles in uni_list:\n",
    "        singles_syn = get_synset(singles,pos_tag_text, 'n')\n",
    "        if singles_syn:\n",
    "            single_synset.append(singles_syn)    \n",
    "    return single_synset\n",
    "\n",
    "#get common parents\n",
    "def get_lcs(uni_list,pos_tag_text):\n",
    "    #get all relevant sysnsets\n",
    "    all_synsets = get_singles_synset(uni_list,pos_tag_text)\n",
    "    list_of_all_lcs =[]\n",
    "    for syn in all_synsets:\n",
    "        for syn2 in all_synsets[all_synsets.index(syn)+1:]:\n",
    "            lcs = syn.lowest_common_hypernyms(syn2)[0]\n",
    "            if lcs not in list_of_all_lcs:\n",
    "                list_of_all_lcs.append(lcs)\n",
    "    return list_of_all_lcs\n",
    "\n",
    "# get themes\n",
    "def get_theme(uni_list,pos_tag_text):\n",
    "    # get common parent\n",
    "    parent_sysnset = get_lcs(uni_list,pos_tag_text)\n",
    "    # filter out absolute top level and get lemma_names\n",
    "    lemma_names =[]\n",
    "    for synset in parent_sysnset:\n",
    "        if synset.min_depth() != 0:\n",
    "            #print(synset)\n",
    "            for each_name in synset.lemma_names():\n",
    "                if each_name not in lemma_names:\n",
    "                    lemma_names.append(each_name)\n",
    "                break\n",
    "    return lemma_names\n",
    "\n",
    "def get_cluster_count(document):\n",
    "    text = str(document['text'][0])\n",
    "    cleantext = get_document_text(text)\n",
    "    tagged_tokens = tag_my_text(tokenize_text_sent(cleantext))\n",
    "    np_list = noun_phrase_finder(tagged_tokens)\n",
    "    top_np = get_top_unigrams(np_list)\n",
    "    themes = get_theme(top_np,tagged_tokens)\n",
    "    return len(themes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Comment clustering\n",
    "def process_document(document):\n",
    "    comments = []\n",
    "    for c in document['comment_list']:\n",
    "        c = c.replace('\\n',' ')\n",
    "        if 'attached' not in c or len(c) > 500:\n",
    "            comments.append(str(c))\n",
    "    return comments\n",
    "\n",
    "# Modified from Brandon Rose:\n",
    "def tokenize_text(text):\n",
    "    tokens = [word.lower() for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    filtered_tokens = []\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token)\n",
    "    return filtered_tokens\n",
    "\n",
    "def stem_text(text):\n",
    "    stemmer = SnowballStemmer('english')\n",
    "    tokens = tokenize_text(text)\n",
    "    stems = [stemmer.stem(t) for t in tokens]\n",
    "    return stems\n",
    "\n",
    "def vectorize_comments(comments):\n",
    "    tfidf_vec = TfidfVectorizer(tokenizer=tokenize_text,\n",
    "                                stop_words='english',\n",
    "                                ngram_range=(1,3),\n",
    "                                min_df=0.2, max_df=0.8,\n",
    "                                max_features=200000)\n",
    "    tfidf_matrix = tfidf_vec.fit_transform(comments)\n",
    "    return tfidf_matrix, tfidf_vec\n",
    "\n",
    "# Modified from Brandon Rose:\n",
    "def vocabulary_frame(text):\n",
    "    tokens = tokenize_text(text)\n",
    "    stems = stem_text(text)\n",
    "    return pd.DataFrame({'words': tokens}, index = stems).drop_duplicates()\n",
    "\n",
    "def extended_vocabulary_frame(texts):\n",
    "    frames = []\n",
    "    for t in texts:\n",
    "        vf = vocabulary_frame(t)\n",
    "        frames.append(vf)\n",
    "    extended = pd.concat(frames).drop_duplicates()\n",
    "    return extended\n",
    "\n",
    "# Modified from Brandon Rose and\n",
    "# http://scikit-learn.org/stable/auto_examples/applications/topics_extraction_with_nmf_lda.html\n",
    "# http://scikit-learn.org/stable/auto_examples/text/document_clustering.html#sphx-glr-auto-examples-text-document-clustering-py\n",
    "def top_words(model, num_clusters, comments, tfidf_vec, n_top_words):\n",
    "    feature_names = tfidf_vec.get_feature_names()\n",
    "    comment_vf = extended_vocabulary_frame(comments)\n",
    "#     return feature_names, comment_vf\n",
    "    order_centroids = model.cluster_centers_.argsort()[:, ::-1] \n",
    "    top_words = []\n",
    "    for i in range(num_clusters):\n",
    "        temp_top_words = []\n",
    "        for j in order_centroids[i, :n_top_words]:\n",
    "            temp_top_words.append(feature_names[j])\n",
    "        top_words.append(temp_top_words)\n",
    "    return top_words\n",
    "\n",
    "def cluster_comments(document, num_clusters):\n",
    "    cluster_dict = {}\n",
    "\n",
    "    comments = process_document(document)\n",
    "    tfidf_matrix, tfidf_vec = vectorize_comments(comments)\n",
    "    \n",
    "    km = KMeans(n_clusters=num_clusters)\n",
    "    km.fit(tfidf_matrix)\n",
    "    clusters = km.labels_.tolist()\n",
    "    \n",
    "    # FOR TESTING\n",
    "    return km, tfidf_vec, comments\n",
    "    \n",
    "#     cluster_center_list = []\n",
    "#     for c in clusters:\n",
    "#         cluster_center_list.append(km.cluster_centers_[c])\n",
    "#     center_distances = paired_distances(tfidf_matrix, cluster_center_list)\n",
    "    \n",
    "#     comment_clusters = {'comment': comments, 'cluster': clusters, 'dist': center_distances}\n",
    "#     comment_frame = pd.DataFrame(comment_clusters, index = [clusters] , columns = ['comment', 'cluster', 'dist'])\n",
    "    \n",
    "#     central_comments = []\n",
    "#     all_comments = []\n",
    "#     for i in range(num_clusters):\n",
    "#         central_comments.append(comment_frame[comment_frame.cluster==i].min().comment)\n",
    "#         all_comments.append(list(comment_frame[comment_frame.cluster==i]['comment']))\n",
    "    \n",
    "#     freq_words = top_words(km, num_clusters, comments, tfidf_vec, 6)\n",
    "    \n",
    "#     cluster_dict['central_comments'] = central_comments\n",
    "#     cluster_dict['all_comments'] = all_comments\n",
    "#     cluster_dict['top_words'] = freq_words\n",
    "    \n",
    "#     return cluster_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doc_list =load(open(\"data/Master2_doc_content\",'rb'))\n",
    "document = doc_list[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BEGIN TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n"
     ]
    }
   ],
   "source": [
    "cluster_num = get_cluster_count(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "km, tfidf_vec, comments = cluster_comments(document, cluster_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "TEST = top_words(km, cluster_num, comments, tfidf_vec, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['rule', 'proposed rule', 'proposed', 'housing', 'hud', 'public'],\n",
       " ['tobacco', 'smoke', 'health', 'smoking', 'housing', 'public'],\n",
       " ['housing', 'public', 'smoke-free', 'public housing', 'health', 'policy'],\n",
       " ['smoke', 'hand', 'second', 'housing', 'building', 'smoking'],\n",
       " ['smoking', 'housing', 'smoke', 'public', 'policy', 'smokers'],\n",
       " ['people', \"n't\", \"'s\", 'smoke', 'smoking', 'smokers'],\n",
       " ['smoke free', 'free', 'smoke', 'housing', 'public', 'policy'],\n",
       " ['smoke', 'secondhand', 'secondhand smoke', 'health', 'housing', 'public'],\n",
       " ['like', 'live', 'smokers', 'support', 'home', 'children']]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"'s\",\n",
       " 'air',\n",
       " 'areas',\n",
       " 'building',\n",
       " 'buildings',\n",
       " 'children',\n",
       " 'department',\n",
       " 'development',\n",
       " 'does',\n",
       " 'exposure',\n",
       " 'free',\n",
       " 'hand',\n",
       " 'health',\n",
       " 'home',\n",
       " 'housing',\n",
       " 'housing urban',\n",
       " 'hud',\n",
       " 'instituting',\n",
       " 'like',\n",
       " 'live',\n",
       " 'living',\n",
       " 'make',\n",
       " \"n't\",\n",
       " 'people',\n",
       " 'policies',\n",
       " 'policy',\n",
       " 'proposed',\n",
       " 'proposed rule',\n",
       " 'public',\n",
       " 'public housing',\n",
       " 'residents',\n",
       " 'rule',\n",
       " 'second',\n",
       " 'secondhand',\n",
       " 'secondhand smoke',\n",
       " 'smoke',\n",
       " 'smoke free',\n",
       " 'smoke-free',\n",
       " 'smokers',\n",
       " 'smoking',\n",
       " 'support',\n",
       " 'time',\n",
       " 'tobacco',\n",
       " 'units',\n",
       " 'urban',\n",
       " 'use',\n",
       " 'years']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>i</th>\n",
       "      <td>i</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>have</th>\n",
       "      <td>have</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>been</th>\n",
       "      <td>been</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fascin</th>\n",
       "      <td>fascinated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>for</th>\n",
       "      <td>for</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             words\n",
       "i                i\n",
       "have          have\n",
       "been          been\n",
       "fascin  fascinated\n",
       "for            for"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comment_vf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### END TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n"
     ]
    }
   ],
   "source": [
    "n = get_cluster_count(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test = cluster_comments(document, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['top_words', 'central_comments', 'all_comments'])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test['central_comments'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
