{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains functions to extract the following information from a document:\n",
    "* Keywords\n",
    "* Top relevant sentences\n",
    "\n",
    "** Input **\n",
    "Reads in data stored in pickle format as extracted using Kinshuk's notebook\n",
    "\n",
    "\n",
    "** Output **\n",
    "`list_of_keywords`,`top_10_relevant_sentences`\n",
    "\n",
    "\n",
    "** Usage **\n",
    "Call `extract_summary` function with the document text as a parameter.\n",
    "\n",
    "\n",
    "** Notes **\n",
    "\n",
    "\n",
    "* I tried to use Texttiling in order to tokenize the text by topics and then use it to extract keywords, themes, etc. However, it did not result in any better quality keywords. A new challenge was that of increased number of keywords, hence beating the purpose of summarizing the text. We decided not to use the algorithm in our processing pipeline.\n",
    "* The function takes into account only unigrams and bigrams while extracting top relevant sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Imports **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pickle import dump, load\n",
    "import nltk\n",
    "from nltk import word_tokenize,FreqDist\n",
    "import re\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.util import ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_document_text(raw_text):\n",
    "    \"\"\" This function takes in raw document text as input which we receive from the API and returns a clean text \n",
    "    of the associated document. It cleans up any HTML code in the text, newline characters, and extracts supplemental\n",
    "    information part of the document.\n",
    "    \n",
    "    INPUT: string\n",
    "    OUTPUT: string\n",
    "    \"\"\"\n",
    "    raw_text = raw_text.replace('\\n',' ')\n",
    "    supp_info_idx = raw_text.find(\"SUPPLEMENTARY INFORMATION:\")\n",
    "    \n",
    "    suppl_info = raw_text[supp_info_idx+26:] # To leave out the string 'Supplementary Information'\n",
    "    # Remove any residual HTML tags in text\n",
    "    cleanr = re.compile('<.*?>')\n",
    "    cleantext = re.sub(cleanr, '', suppl_info)\n",
    "    \n",
    "    return suppl_info\n",
    "\n",
    "def get_keywords(clean_corpus):\n",
    "    \"\"\" This function takes in a clean corpus as input and extracts most important keywords and top 10% of relevant \n",
    "    sentences from the text.\n",
    "    \n",
    "    INPUT: string\n",
    "    OUTPUT: List of tuples: [(list_of_keywords,list_of_sentences)]\n",
    "    \"\"\"\n",
    "    \n",
    "    tagged_tokens = tag_my_text(tokenize_text_sent(clean_corpus))\n",
    "    grand_list = get_top_np(noun_phrase_finder(tagged_tokens))\n",
    "    \n",
    "    return grand_list\n",
    "\n",
    "def tokenize_text(corpus):\n",
    "    pattern = r'''(?x)    # set flag to allow verbose regexps\n",
    "    (([A-Z]\\.)+)       # abbreviations, e.g. B.C.\n",
    "    |(\\w+([-']\\w+)*)       # words with optional internal hyphens e.g. after-ages or author's\n",
    "    '''\n",
    "    tokens = nltk.regexp_tokenize(corpus,pattern)\n",
    "    all_token = [word.lower() for token in tokens for word in token if word != \"\" \n",
    "                 and word[0] != \"'\" and word[0] != \"-\"]\n",
    "    return all_token\n",
    "\n",
    "def tokenize_text_sent(corpus):\n",
    "    sent_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "    raw_sents = sent_tokenizer.tokenize(corpus) # Split text into sentences    \n",
    "    return [tokenize_text(sent) for sent in raw_sents]\n",
    "\n",
    "def tag_my_text(sents):\n",
    "    return [nltk.pos_tag(sent) for sent in sents]\n",
    "\n",
    "#Chunk noun phrases in tree \n",
    "def noun_phrase_chunker():\n",
    "    grammar = r\"\"\"\n",
    "    NP: {<DT|PP\\$>?<JJ>*<NN>}   # chunk determiner/possessive, adjectives and noun\n",
    "    \"\"\"\n",
    "    cp = nltk.RegexpParser(grammar)\n",
    "    return cp\n",
    "\n",
    "#Extract only the NP marked phrases from the parse tree, that is the chunk we defined\n",
    "def noun_phrase_extractor(sentences, chunker):\n",
    "    res = []\n",
    "    for sent in sentences:\n",
    "        tree = chunker.parse(sent)\n",
    "        for subtree in tree.subtrees():\n",
    "            if subtree.label() == 'NP' : \n",
    "                res.append(subtree[0:len(subtree)])\n",
    "                #res.append(subtree[0])\n",
    "                #print(subtree)\n",
    "    return res\n",
    "\n",
    "def noun_phrase_finder(tagged_text):\n",
    "    all_proper_noun = noun_phrase_extractor(tagged_text,noun_phrase_chunker()) \n",
    "    #does not literally mean proper noun. Chunker only extracts common noun\n",
    "    noun_phrase_list = []                                                      \n",
    "    #noun_phrase_string_list =[]\n",
    "    for noun_phrase in all_proper_noun:\n",
    "        if len(noun_phrase) > 0: #this means where the size of the phrase is greater than 1\n",
    "            small_list =[]\n",
    "            for (word,tag) in noun_phrase:\n",
    "                small_list.append(word)\n",
    "            noun_phrase_list.append(small_list)\n",
    "            #noun_phrase_string_list.append(' '.join(small_list))\n",
    "    return noun_phrase_list\n",
    "\n",
    "#get frequency dist of different length in all the noun phrases extracted. \n",
    "#Something of the form {1:45,2:23} - how many 1phrased and 2 phrased chunks I have etc.\n",
    "def get_length_np(nounPhrase):\n",
    "    np_length={}\n",
    "    for inner_np in nounPhrase:\n",
    "        np_length[len(inner_np)] = np_length.get(len(inner_np),0) + 1\n",
    "    return np_length\n",
    "\n",
    "#get freq dist obj for noun phrase of different lengths\n",
    "def find_freq(nested_list,nest_len):\n",
    "    #from nltk.probability import FreqDist\n",
    "    fdist_list =[]\n",
    "    for inner_np in nested_list:\n",
    "        if len(inner_np) == nest_len:\n",
    "            fdist_list.append(' '.join(inner_np))\n",
    "    fdist = FreqDist(fdist_list)\n",
    "    return fdist\n",
    "\n",
    "#make a grand list of top occuring noun phrases of different sizes\n",
    "def get_top_np(np):\n",
    "    master_common_list=[]\n",
    "    len_list =get_length_np(np).keys()\n",
    "    for item in len_list:\n",
    "        fdist_np = find_freq(np,item)\n",
    "        top = fdist_np.most_common(15) \n",
    "        top_list = []\n",
    "        for w,c in top:\n",
    "            if c >= 10:\n",
    "#                 print (w)\n",
    "                top_list.append((w,c))\n",
    "                #top.remove((w,c))\n",
    "        if len(top_list) > 0:\n",
    "            master_common_list.append(top_list)\n",
    "    return master_common_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_top_sents(corpus,keywords_list):\n",
    "    sentence_list = get_sentences(corpus)\n",
    "    indexed_sents = sentence_indexing(sentence_list) # This is so that we can re-order most relevant sentences later\n",
    "    \n",
    "    sentence_length_scores = get_sentence_lengths(sentence_list)\n",
    "    keyphrase_scores = get_keyphrase_scores(corpus,sentence_list)\n",
    "    \n",
    "    sent_scores = [s+c for s,c in zip(sentence_length_scores,keyphrase_scores)]\n",
    "    idx_sent_scores = [(s,c) for s,c in zip(indexed_sents,sent_scores)]\n",
    "    sorted_sents = sorted(idx_sent_scores,key=lambda sent: sent[1],reverse=True)\n",
    "    \n",
    "    # Keep top 10% of the sentences, or top 10 whichever is less\n",
    "    top_10 = int(len(sorted_sents) * 0.1)\n",
    "    if top_10 > 10:\n",
    "        top_10 = 10\n",
    "    x = sorted_sents[:top_10]\n",
    "    top_list = [item[0] for item in x]\n",
    "    sorted_top_list = sorted(top_list,key=lambda sent:sent[1],reverse=False)\n",
    "    sorted_top_list = [sent[0] for sent in sorted_top_list]\n",
    "    \n",
    "    return sorted_top_list\n",
    "    \n",
    "def get_sentences(corpus):\n",
    "    # First, tokenize the corpus into sentences\n",
    "    sent_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "    raw_sents = sent_tokenizer.tokenize(corpus)\n",
    "    \n",
    "    return raw_sents\n",
    "\n",
    "def sentence_indexing(sent_list):\n",
    "    indexed_sents = []\n",
    "    for idx,sent in enumerate(sent_list):\n",
    "        indexed_sents.append((sent,idx))\n",
    "    return indexed_sents    \n",
    "\n",
    "def get_sentence_lengths(sent_list):\n",
    "    sent_length = []\n",
    "    for s in sent_list:\n",
    "        sent_length.append(len(s.split(' ')))\n",
    "    \n",
    "    return sent_length\n",
    "\n",
    "def get_keyphrase_scores(corpus,sent_list):\n",
    "    keywords = get_keywords(corpus) # This gives us a list containing unigrams at index 0 and bigrams at index 1,etc\n",
    "    \n",
    "    unigrams = [item[0] for item in keywords[0]]\n",
    "    bigrams = [item[0] for item in keywords[1]]\n",
    "\n",
    "    unigram_scores = get_unigram_scores(unigrams,sent_list)\n",
    "    bigram_scores = get_bigram_scores(bigrams,sent_list)\n",
    "\n",
    "    sent_feature_import = [a+b for a,b in zip(unigram_scores,bigram_scores)]\n",
    "    \n",
    "    return sent_feature_import\n",
    "\n",
    "def get_unigram_scores(unigram_list,sent_list):\n",
    "    occurence_list = []\n",
    "    for s in sent_list:\n",
    "        words = s.split(' ')\n",
    "        occurence_count = 0\n",
    "        for w in words:\n",
    "            if w.lower() in unigram_list or w.lower() in ['complaint','concern','documented','evidence','warn']:\n",
    "                occurence_count += 1\n",
    "        occurence_list.append(occurence_count)\n",
    "        \n",
    "    return occurence_list\n",
    "\n",
    "def get_bigram_scores(bigram_list,sent_list):\n",
    "    occurence_list = []\n",
    "    for s in sent_list:\n",
    "        # create bigrams\n",
    "        token=nltk.word_tokenize(s)\n",
    "        bigram_phrases = ngrams(token,2)\n",
    "        occurence_count = 0\n",
    "        for w in bigram_phrases:\n",
    "            w = [word.lower() for word in w]\n",
    "            if ' '.join(w) in bigram_list:\n",
    "                occurence_count += 1\n",
    "        occurence_list.append(occurence_count)\n",
    "        \n",
    "    return occurence_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def extract_summary(text):\n",
    "    clean_text = get_document_text(text)\n",
    "    keywords = get_keywords(clean_text)\n",
    "    top = get_top_sents(clean_text,keywords)\n",
    "    \n",
    "    return keywords,top"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Test **\n",
    "\n",
    "Testing out the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Test\n",
    "doc_list =load(open(\"data/Proxima_doc_content\",'rb'))\n",
    "document = doc_list[0]\n",
    "document_text = []\n",
    "for item in document['text']:\n",
    "    document_text.append(str(item))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "keywords,sentences = extract_summary(document_text[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Output **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('trail', 35),\n",
       "  ('plan', 22),\n",
       "  ('use', 19),\n",
       "  ('bicycle', 17),\n",
       "  ('management', 12),\n",
       "  ('cuyahoga', 11),\n",
       "  ('bullet', 10),\n",
       "  ('executive', 10),\n",
       "  ('order', 10)],\n",
       " [('the park', 29), ('this rule', 16)]]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"  Background  Legislation and Purposes of Cuyahoga Valley National Park      On December 27, 1974, President Gerald Ford signed Public Law 93- 555 creating Cuyahoga Valley National Recreation Area for the purpose  of ``preserving and protecting for public use and enjoyment, the  historic, scenic, natural, and recreational values of the Cuyahoga  River and the adjacent lands of the Cuyahoga Valley and for the purpose  of providing for the maintenance of needed recreational open space  necessary to the urban environment.''\",\n",
       " \"The goals of the 2009 Plan/EIS were to develop a trail network  that:     <bullet> Provides experiences for a variety of trail users;     <bullet> shares the historic, scenic, natural and recreational  significance of the Park;     <bullet> minimizes impacts to the park's historic, scenic, natural  and recreational resources;     <bullet> can be sustained; and     <bullet> engages cooperative partnerships that contribute to the  success of the Park's trail network.\",\n",
       " 'To provide users  with additional recreational bicycling opportunities, this proposed  rule would authorize the Superintendent to designate any of the  following trails as routes for bicycle use:  ----------------------------------------------------------------------------------------------------------------                                     Approximate            Trail name                  length          Surface type        Usage type           Description ---------------------------------------------------------------------------------------------------------------- East Rim.......................  10 miles.........  Natural surface..  Off-road, single-  Approximately ten                                                                         track bicycle.',\n",
       " 'miles of a loop                                                                                            system trail of                                                                                            varying distances                                                                                            along the east                                                                                            central portion of                                                                                            the Park, north of                                                                                            Old Akron-Peninsula                                                                                            Road and south of                                                                                            Brandywine Falls                                                                                            trailhead, near the                                                                                            Krecjic Restoration                                                                                            Site.',\n",
       " 'Located west of Blue                                                                                            Hen Falls, near the                                                                                            existing Kurowski                                                                                            fields and linking to                                                                                            the existing Buckeye                                                                                            Trail.',\n",
       " 'Crushed gravel...  Multi-purpose....  Extension of existing                                                                                            Old Carriage Road                                                                                            connector to existing                                                                                            Bike and Hike Trail.',\n",
       " 'Crushed gravel...  Multi-purpose....  New connector from                                                                                            existing Bike and                                                                                            Hike Trail to                                                                                            existing Towpath                                                                                            Trail on south side                                                                                            of Highland Road,                                                                                            extending on the                                                                                            north side of                                                                                            Highland Road from                                                                                            Towpath to the Vaughn                                                                                            overflow parking                                                                                            area.',\n",
       " \"----------------------------------------------------------------------------------------------------------------      After trail construction is completed, but before a trail is  designated for bicycle use, the Superintendent would be required to  issue a written determination that the route is open for public use and  that such bicycle use is consistent with the 2013 park plan for bicycle  use, including implementation of the park's sustainable trail  guidelines with monitoring and mitigation through adaptive management.\",\n",
       " '(1) The Superintendent may designate routes or  portions of routes for bicycle use on the following trails:     (i) East Rim (approximately 10 miles);     (ii) High Meadow Trail (approximately 3.1 miles);     (iii) Old Carriage Connector Trail (approximately 0.35 miles); and     (iv) Highland Connector Trail (approximately 1.0 mile).',\n",
       " \"(2) After trail construction is complete:     (i) To designate a bicycle route, the Superintendent must make a  written determination that:     (A) The route is open for public use; and     (B) Bicycle use is consistent with the protection of the park  area's natural, scenic and aesthetic values, safety considerations, and  management objectives, and will not disturb wildlife or park resources.\"]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
