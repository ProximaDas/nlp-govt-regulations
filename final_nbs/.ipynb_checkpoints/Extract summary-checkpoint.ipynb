{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project - Notice and comment\n",
    "Project by Jason Danker, Proxima DasMohapatra, Ankur Kumar, Emily Witt and Kinshuk\n",
    "## Notebook title - Summarize regulation text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Overview**: \n",
    "This notebook contains functions to extract the following information from a document:\n",
    "* Keywords\n",
    "* Top relevant sentences\n",
    "\n",
    "** Input **\n",
    "Reads in data stored in pickle format as extracted using Kinshuk's notebook\n",
    "\n",
    "\n",
    "** Output **\n",
    "`list_of_keywords`,`top_10_relevant_sentences`\n",
    "\n",
    "\n",
    "** Usage **\n",
    "Call `extract_summary` function with the document text as a parameter.\n",
    "\n",
    "\n",
    "** Notes **\n",
    "* We tried to use Texttiling in order to tokenize the text by topics and then use it to extract keywords, themes, etc. However, it did not result in any better quality keywords. A new challenge was that of increased number of keywords, hence beating the purpose of summarizing the text. We decided not to use the algorithm in our processing pipeline.\n",
    "* The function takes into account only unigrams and bigrams while extracting top relevant sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import List\n",
    "from pickle import dump, load\n",
    "import nltk\n",
    "from nltk import word_tokenize,FreqDist\n",
    "import re\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.util import ngrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This Cell has code for key word extraction. The result is directly used as top keywords as well fed into sentence extraction algorith.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Input: Raw text \n",
    "#Output: supplementry information part and summary part \n",
    "#Explanation: The function cleans raw text by removing newline character, return character and also html tags. \n",
    "#            Then we seperate the text into summary and supplementry information\n",
    "def get_document_text(raw_text):\n",
    "    \"\"\" This function takes in raw document text as input which we receive from the API and returns a clean text \n",
    "    of the associated document. It cleans up any HTML code in the text, newline characters, and extracts supplemental\n",
    "    information part of the document.\n",
    "    \n",
    "    INPUT: string\n",
    "    OUTPUT: string\n",
    "    \"\"\"\n",
    "    raw_text = raw_text.replace('\\n',' ')\n",
    "    raw_text = raw_text.replace('*','') # added\n",
    "    raw_text = raw_text.replace('\\r',' ') # added\n",
    "    supp_info_idx = raw_text.find(\"SUPPLEMENTARY INFORMATION:\")\n",
    "    summary_idx = raw_text.find(\"SUMMARY:\")\n",
    "    dates_idx = raw_text.find(\"DATES:\")\n",
    "    suppl_info = raw_text[supp_info_idx+26:] # To leave out the string 'Supplementary Information'\n",
    "    summary = raw_text[summary_idx+8:dates_idx]\n",
    "    # Remove any residual HTML tags in text\n",
    "    cleanr = re.compile('<.*?>')\n",
    "    cleantext = re.sub(cleanr, '', suppl_info)\n",
    "    cleansummary = re.sub(cleanr, '', summary)\n",
    "    return cleantext, cleansummary\n",
    "\n",
    "#Input: text sentence \n",
    "#Output: List of tokens\n",
    "#Explanation: uses regex and a regex tokenizer\n",
    "\n",
    "def tokenize_text(corpus):\n",
    "    pattern = r'''(?x)    # set flag to allow verbose regexps\n",
    "    (([A-Z]\\.)+)       # abbreviations, e.g. B.C.\n",
    "    |(\\w+([-']\\w+)*)       # words with optional internal hyphens e.g. after-ages or author's\n",
    "    '''\n",
    "    tokens = nltk.regexp_tokenize(corpus,pattern)\n",
    "    all_token = [word.lower() for token in tokens for word in token if word != \"\" \n",
    "                 and word[0] != \"'\" and word[0] != \"-\"]\n",
    "    return all_token\n",
    "\n",
    "#Input: corpus\n",
    "#Output: List of tokens in form of list of sentence which contains list of tokesn\n",
    "#Explanation: first tokenize sentences and run it through word tokenizer\n",
    "\n",
    "def tokenize_text_sent(corpus):\n",
    "    sent_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "    raw_sents = sent_tokenizer.tokenize(corpus) # Split text into sentences    \n",
    "    return [tokenize_text(sent) for sent in raw_sents]\n",
    "\n",
    "#Input: list of tokenized word in sentences (nested list)\n",
    "#Output: tagged tokens\n",
    "#Explanation: uses nlp default tokenizer\n",
    "def tag_my_text(sents):\n",
    "    return [nltk.pos_tag(sent) for sent in sents]\n",
    "\n",
    "#Input: None\n",
    "#Output: A noun phrase chunker based on regex\n",
    "#Explanation: Chunk noun phrases in tree\n",
    "def noun_phrase_chunker():\n",
    "    grammar = r\"\"\"\n",
    "    NP: {<DT|PP\\$>?<JJ>*<NN>}   # chunk determiner/possessive, adjectives and noun\n",
    "    \"\"\"\n",
    "    cp = nltk.RegexpParser(grammar)\n",
    "    return cp\n",
    "\n",
    "#Input: tagged sentences list and noun phrase chunker object\n",
    "#Output: all noun phrase chunks\n",
    "#Explanation: Extract only the NP marked phrases from the parse tree, that is the chunk we defined\n",
    "def noun_phrase_extractor(sentences, chunker):\n",
    "    res = []\n",
    "    for sent in sentences:\n",
    "        tree = chunker.parse(sent)\n",
    "        for subtree in tree.subtrees():\n",
    "            if subtree.label() == 'NP' : \n",
    "                res.append(subtree[0:len(subtree)])\n",
    "                #res.append(subtree[0])\n",
    "                #print(subtree)\n",
    "    return res\n",
    "\n",
    "\n",
    "#Input: tagged text\n",
    "#Output: A noun phrase list \n",
    "#Explanation: Only add take the noun phrase list and just extract the noun phrase itself \n",
    "def noun_phrase_finder(tagged_text):\n",
    "    all_proper_noun = noun_phrase_extractor(tagged_text,noun_phrase_chunker()) \n",
    "    #does not literally mean proper noun. Chunker only extracts common noun\n",
    "    noun_phrase_list = []                                                      \n",
    "    #noun_phrase_string_list =[]\n",
    "    for noun_phrase in all_proper_noun:\n",
    "        if len(noun_phrase) > 0: \n",
    "            small_list =[]\n",
    "            for (word,tag) in noun_phrase:\n",
    "                small_list.append(word)\n",
    "            noun_phrase_list.append(small_list)\n",
    "            #noun_phrase_string_list.append(' '.join(small_list))\n",
    "    return noun_phrase_list\n",
    "\n",
    "#Input: noun phrase list\n",
    "#Output: dictionary of len np and count of nps of that length\n",
    "#Explanation: Get frequency dist of different length in all the noun phrases extracted. \n",
    "#             Something of the form {1:45,2:23} - how many 1phrased and 2 phrased chunks I have etc.\n",
    "def get_length_np(nounPhrase):\n",
    "    np_length={}\n",
    "    for inner_np in nounPhrase:\n",
    "        np_length[len(inner_np)] = np_length.get(len(inner_np),0) + 1\n",
    "    return np_length\n",
    "\n",
    "#Input: Nested list of all noun phrases and the length of np \n",
    "#Output: a frequecy distribution object\n",
    "#Explanation: get freq dist obj for noun phrase of different lengths\n",
    "def find_freq(nested_list,nest_len):\n",
    "    #from nltk.probability import FreqDist\n",
    "    fdist_list =[]\n",
    "    for inner_np in nested_list:\n",
    "        if len(inner_np) == nest_len:\n",
    "            fdist_list.append(' '.join(inner_np))\n",
    "    fdist = FreqDist(fdist_list)\n",
    "    return fdist\n",
    "\n",
    "#Input: Noun Phrase list\n",
    "#Output: Master list which is list of list of top np of different size\n",
    "#Explanation: Make a grand list of top occuring noun phrases of different sizes \n",
    "#             **For testing purpose only. Wont be used**\n",
    "def get_top_np(np):\n",
    "    master_common_list=[]\n",
    "    len_list =get_length_np(np).keys()\n",
    "    for item in len_list:\n",
    "        fdist_np = find_freq(np,item)\n",
    "        master_common_list.append(fdist_np.most_common(15))\n",
    "    return master_common_list\n",
    "\n",
    "#Input: Noun Phrase list\n",
    "#Output: Top unigrams \n",
    "#Explanation: Top 30% of unigrams which have word length of more than 3\n",
    "def get_top_unigrams(np):\n",
    "    unigrams = []\n",
    "    for item in np:\n",
    "        if len(item) ==  1:\n",
    "            unigrams.append(item)\n",
    "    fdist_uni = find_freq(np,1)\n",
    "    uni_list = fdist_uni.most_common()\n",
    "    threshold = 0.3 * len(unigrams)\n",
    "    top = []\n",
    "    s = 0\n",
    "    for word,count in uni_list:\n",
    "        if(len(word)>3):\n",
    "            top.append(word)\n",
    "            s += count\n",
    "            if s > threshold:\n",
    "                break      \n",
    "    return top\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This cell has the sentence extraction algorithm and takes as input keywords from above algorithm **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Input: Corpus adn list of keywords\n",
    "#Output: Sorted top sentences\n",
    "#Explanation: It takes the corpus and extracts top sentences based on \n",
    "#            the stepped length of the sentences, keyword occurance and seleted word occurence.\n",
    "#            Penalized if tables exist.\n",
    "#            It then extracts the top 4 sentences based on score and then sorts them by index in text.\n",
    "def get_top_sents(corpus,keywords_list):\n",
    "    sentence_list = get_sentences(corpus)\n",
    "    indexed_sents = sentence_indexing(sentence_list) # This is so that we can re-order most relevant sentences later\n",
    "    \n",
    "    table_scores = handle_tables(sentence_list)\n",
    "    sentence_length_scores = get_sentence_lengths(sentence_list)\n",
    "    keyphrase_scores = get_keyphrase_scores(corpus,sentence_list, keywords_list)\n",
    "    #stepped length\n",
    "    stepped_sentence_length =[]\n",
    "    for each_score in sentence_length_scores:\n",
    "        s = each_score//10\n",
    "        if s>10:\n",
    "            s = 10\n",
    "        stepped_sentence_length.append(s)\n",
    "        \n",
    "    #sent_scores = [s+c for s,c in zip(sentence_length_scores,keyphrase_scores)] #original score = keyphrase +length\n",
    "    #sent_scores = [c/s for s,c in zip(sentence_length_scores,keyphrase_scores)] #score = ratio of keyphrase /length\n",
    "    sent_scores = [c+s+t for s,c,t in zip(stepped_sentence_length,keyphrase_scores,table_scores)] #score = key phrase + stepped length\n",
    "    #sent_scores = [s+(k/l)*100 for s,l,k in zip(stepped_sentence_length,sentence_length_scores,keyphrase_scores)] #score = key phrase + stepped length\n",
    "    \n",
    "    idx_sent_scores = [(s,c) for s,c in zip(indexed_sents,sent_scores)]\n",
    "    sorted_sents = sorted(idx_sent_scores,key=lambda sent: sent[1],reverse=True)\n",
    "    \n",
    "    # Keep top 10% of the sentences, or top 10 whichever is less\n",
    "    top_10 = int(len(sorted_sents) * 0.1)\n",
    "    if top_10 > 4: # changed from 10 to 4\n",
    "        top_10 = 4\n",
    "    x = sorted_sents[:top_10]\n",
    "    top_list = [item[0] for item in x]\n",
    "    sorted_top_list = sorted(top_list,key=lambda sent:sent[1],reverse=False)\n",
    "    sorted_top_list = [sent[0] for sent in sorted_top_list]\n",
    "    \n",
    "    return sorted_top_list\n",
    "\n",
    "#Input: Corpus\n",
    "#Output: Clean list of sentecnes\n",
    "#Explanation: We tokenize setence as well as remove reference number in form /1/, dashes in form --- to mark end of section \n",
    "#             and page number in form [[Page 123]]     \n",
    "def get_sentences(corpus):\n",
    "    # First, tokenize the corpus into sentences\n",
    "    sent_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "    raw_sents = sent_tokenizer.tokenize(corpus)\n",
    "    clean_sent =[]\n",
    "    for sent in raw_sents:\n",
    "        clean_sent.append(re.sub(r\"\\[+Page\\s*\\d+\\]+|\\\\+\\d+\\\\+|-+\",'',sent))\n",
    "    return clean_sent\n",
    "    #return raw_sents\n",
    "\n",
    "#Input: Sentence list\n",
    "#Output: List of sentence as a tuple with index number\n",
    "#Explanation:  This is needed to ultimately arrange them by order of occurence in main text so that they are coherent when read in order\n",
    "def sentence_indexing(sent_list):\n",
    "    indexed_sents = []\n",
    "    for idx,sent in enumerate(sent_list):\n",
    "        indexed_sents.append((sent,idx))\n",
    "    return indexed_sents    \n",
    "\n",
    "\n",
    "#Input: Sentence list\n",
    "#Output: list of length of sentences\n",
    "#Explanation: This is a criteria to score sentences\n",
    "def get_sentence_lengths(sent_list):\n",
    "    sent_length = []\n",
    "    for s in sent_list:\n",
    "        sent_length.append(len(s.split(' ')))\n",
    "    \n",
    "    return sent_length\n",
    "\n",
    "#Input: Corpus, Sentence list, keyword list\n",
    "#Output: List of sentence keyword score\n",
    "#Explanation: This is a criteria to score sentences. Scored on basis of occurence of top unigrams and bigrams\n",
    "def get_keyphrase_scores(corpus,sent_list, keywords):\n",
    "    #keywords = get_keywords(corpus) # This gives us a list containing unigrams at index 0 and bigrams at index 1,etc\n",
    "    \n",
    "    unigrams = [item[0] for item in keywords[0]]\n",
    "    bigrams = [item[0] for item in keywords[1]]\n",
    "\n",
    "    unigram_scores = get_unigram_scores(unigrams,sent_list)\n",
    "    bigram_scores = get_bigram_scores(bigrams,sent_list)\n",
    "\n",
    "    sent_feature_import = [a+b for a,b in zip(unigram_scores,bigram_scores)]\n",
    "    \n",
    "    return sent_feature_import\n",
    "\n",
    "#Input: Unigram list, Sentence list\n",
    "#Output: List of sentence unigram score\n",
    "#Explanation: Scored on basis of occurence of top unigrams and selected words\n",
    "def get_unigram_scores(unigram_list,sent_list):\n",
    "    occurence_list = []\n",
    "    for s in sent_list:\n",
    "        words = s.split(' ')\n",
    "        occurence_count = 0\n",
    "        for w in words:\n",
    "            if w.lower() in unigram_list or w.lower() in ['complaint','concern','documented','evidence','warn']:\n",
    "                occurence_count += 1\n",
    "        occurence_list.append(occurence_count)\n",
    "        \n",
    "    return occurence_list\n",
    "\n",
    "#Input: Sentence list\n",
    "#Output: List of sentence table score\n",
    "#Explanation: Scored on basis of occurence of table. A whole table is treated as sentence and is not very interesting or useful\n",
    "#             A table is mostly numeric so the assumption is if the frequency of number in a sentence is more than 9% it is penalized as it is table\n",
    "#             9% is used after testing it on various tables. Penalization is heavy because we never want to show it \n",
    "#             no matter how many keyword it has or how big it is (table senetcence are usually big).\n",
    "def handle_tables(sent_list):\n",
    "    scores = []\n",
    "    for sent in sent_list:\n",
    "        ss = re.sub(r\"\\s+\",' ',sent)\n",
    "        dots = ss.count('.')\n",
    "        numbers = ss.count('1') +sent.count('2') +sent.count('3')+ sent.count('4') +sent.count('5')+sent.count('6')+sent.count('7')+sent.count('8')+sent.count('9')+sent.count('10')\n",
    "        sent_len = len(ss)\n",
    "        if (dots+numbers)/sent_len >= 0.09:\n",
    "            scores.append(-100)\n",
    "        else:\n",
    "            scores.append(0)\n",
    "    return scores\n",
    "            \n",
    "#Input: Bigram list, Sentence list\n",
    "#Output: List of sentence bigram score\n",
    "#Explanation: Scored on basis of occurence of top bigrams\n",
    "def get_bigram_scores(bigram_list,sent_list):\n",
    "    occurence_list = []\n",
    "    for s in sent_list:\n",
    "        # create bigrams\n",
    "        token=nltk.word_tokenize(s)\n",
    "        bigram_phrases = ngrams(token,2)\n",
    "        occurence_count = 0\n",
    "        for w in bigram_phrases:\n",
    "            w = [word.lower() for word in w]\n",
    "            if ' '.join(w) in bigram_list:\n",
    "                occurence_count += 1\n",
    "        occurence_list.append(occurence_count)\n",
    "        \n",
    "    return occurence_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Input: text from data file\n",
    "#Output: top keywords, top sentences, summary from document\n",
    "#Explanation: Pakgesa all the functions above together. \n",
    "def extract_summary(text):\n",
    "    clean_text, clean_summary = get_document_text(text)\n",
    "    tagged_tokens = tag_my_text(tokenize_text_sent(clean_text))\n",
    "    np_list = noun_phrase_finder(tagged_tokens)\n",
    "    keywords = get_top_np(np_list)\n",
    "    top_np = get_top_unigrams(np_list)  \n",
    "    #keywords = get_keywords(clean_text)\n",
    "    top_sent = get_top_sents(clean_text,keywords)\n",
    "    \n",
    "    return top_np,top_sent,clean_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Test **\n",
    "\n",
    "Testing out the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Test\n",
    "doc_list =load(open(\"data/Master_doc_content\",'rb'))\n",
    "document = doc_list[3]\n",
    "document_text = []\n",
    "for item in document['text']:\n",
    "    document_text.append(str(item))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "key,sentences, summary = extract_summary(document_text[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Output **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['room',\n",
       " 'research',\n",
       " 'usage',\n",
       " 'labor',\n",
       " 'part',\n",
       " 'nara',\n",
       " 'washington',\n",
       " 'march',\n",
       " 'college',\n",
       " 'monday',\n",
       " 'amend']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Research Room Hours in DC Area Facilities        Our research center and Central Research Room in the National   Archives Building and the research rooms at the National Archives at   College Park facility are currently open for research Monday through   Friday from 8:45 a.m. to 5 p.m.; on Tuesday, Thursday and Friday   evenings from 5 p.m. to 9 p.m.; and Saturdays from 8:45 a.m. to 4:45   p.m.\n",
      "\n",
      "\n",
      "Researchers who conduct research in original archival records in the   evening or on Saturday currently must make a reference request in  person before 3:30 on weekdays to have the records identified and   retrieved from the stack areas for their research use; no records are   retrieved during those extended hours.\n",
      "\n",
      "\n",
      "Currently the National Archives Experience (our Washington DC   museum) including the Rotunda for the Charters of Freedom (displaying   the Declaration of Independence, Constitution, and Bill of Rights) is   open to the public as follows:       The day after Labor Day through March 31, 10 a.m. to 5:30   p.m. (closed on December 25);       April 1 through the Friday before Memorial Day, 10 a.m. to   7 p.m.;       Memorial Day weekend through Labor Day, 10 a.m. to 9 p.m.    We are revising Sec.\n",
      "\n",
      "\n",
      "There are many actions   that must be carried out in order to provide access to archival records   in NARA research rooms; to assist researchers via online descriptions   in our Archival Research Catalog or via written reference requests; and   to develop public programs and exhibits.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for sent in sentences:\n",
    "    print(sent)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' This document provides an additional 15 days for interested   persons to submit comments on the proposed rule to amend the Customs   and Border Protection (CBP) regulations pertaining to pilots of any   private aircraft arriving in the United States from a foreign port or   location or departing the United States for a foreign port or location.   The proposed rule was published in the Federal Register on September   18, 2007, and the comment period was scheduled to expire on November   19, 2007.    '"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing to a file as JSON\n",
    "After testing we run it ion every document in data file and save in JSON formatat. We chose this formata as it would be easier to include on the demo website. \n",
    "\n",
    "The dictionary format in JSON will be like - \n",
    "```\n",
    "{\"data\": [\n",
    "        \"keywords\": [],\n",
    "        \"sentences\": [],\n",
    "        \"summary\": <string>,\n",
    "        \"doc_id\": <string>,\n",
    "        \"doc_title\": <string>\n",
    "    ],\n",
    "    ....\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doc_list1 =load(open(\"data/Master_doc_content\",'rb'))\n",
    "doc_list2 = load(open(\"data/Master2_doc_content\",'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "doc_id1 = [\"FAA-2010-1127-0001\",\"USCBP-2007-0064-1986\",\"FMCSA-2015-0419-0001\",\"NARA-06-0007-0001\",\"APHIS-2006-0041-0001\",\"EBSA-2012-0031-0001\",\"IRS-2010-0009-0001\",\"BOR-2008-0004-0001\",\"OSHA-2013-0023-1443\",\"DOL-2016-0001-0001\",\"NRC-2015-0057-0086\",\"CMS-2010-0259-0001\",\"CMS-2009-0008-0003\",\"CMS-2009-0038-0002\",\"NPS-2014-0005-000\",\"BIS-2015-0011-0001\",\"HUD-2011-0056-0019\",\"HUD-2011-0014-0001\",\"OCC-2011-0002-0001\",\"ACF-2015-0008-0124\",\"ETA-2008-0003-0001\",\"CMS-2012-0152-0004\",\"CFPB-2013-0033-0001\",\"USCIS-2016-0001-0001\",\"FMCSA-2011-0146-0001\",\"USCG-2013-0915-0001\",\"NHTSA-2012-0177-0001\",\"USCBP-2005-0005-0001\"]\n",
    "doc_id2 = [\"HUD-2015-0101-0001\",\"ACF-2010-0003-0001\",\"NPS-2015-0008-0001\",\"FAR-2014-0025-0026\",\"CFPB-2013-0002-0001\",\"DOS-2010-0035-0001\",\"USCG-2013-0915-0001\",\"SBA-2010-0001-0001\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doc_title1 = [\"Photo Requirements for Pilot Certificates\",\n",
    "             \"Advance Information on Private Aircraft Arriving and Departing the United States\",\n",
    "             \"Evaluation of Safety Sensitive Personnel for Moderate-to-Severe Obstructive Sleep Apnea\",\n",
    "             \"Changes in NARA Research Room and Museum Hours\",\n",
    "             \"Bovine Spongiform Encephalopathy; Minimal-Risk Regions; Importation of Live Bovines and Products Derived From Bovines\",\n",
    "             \"Incentives for Nondiscriminatory Wellness Programs in Group Health Plans\",\n",
    "             \"Furnishing Identifying Number of Tax Return Preparer\",\n",
    "             \"Use of Bureau of Reclamation Land, Facilities, and Waterbodies\",\n",
    "             \"Improve Tracking of Workplace Injuries and Illnesses\",\n",
    "             \"Implementation of the Nondiscrimination and Equal Opportunity Provisions of the Workforce Innovation and Opportunity Act\",\n",
    "             \"Linear No-Threshold Model and Standards for Protection Against Radiation; Extension of Comment Period\",\n",
    "             \"Medicare Program: Accountable Care Organizations and the Medicare Shared Saving Program\",\n",
    "             \"Medicare Program: Changes to the Competitive Acquisition of Certain Durable Medical Equipment, Prosthetics, Orthotics and Supplies (DMEPOS) by Certain Provisions of the Medicare Improvements for Patients and Providers Act of 2008 (MIPPA)\",\n",
    "             \"Medicare Program: Inpatient Rehabilitation Facility Prospective Payment System for Federal Fiscal Year 2010 \",\n",
    "             \"Special Regulations: Areas of the National Park System, Cuyahoga Valley National Park, Bicycling\",\n",
    "             \"Wassenaar Arrangement Plenary Agreements Implementation; Intrusion and Surveillance Items\",\n",
    "             \"Credit Risk Retention 2\",\n",
    "             \"FR 5359–P–01 Equal Access to Housing in HUD Programs Regardless of Sexual Orientation or Gender Identity \",\n",
    "             \"Credit Risk Retention\",\n",
    "             \"Head Start Performance Standards; Extension of Comment Period\",\n",
    "             \"Senior Community Service Employment Program\",\n",
    "             \"Patient Protection and Affordable Care Act: Benefit and Payment Parameters for 2014\",\n",
    "             \"Debt Collection (Regulation F)\",\n",
    "             \"U.S. Citizenship and Immigration Services Fee Schedule\",\n",
    "             \"Applicability of Regulations to Operators of Certain Farm Vehicles and Off-Road Agricultural Equipment\",\n",
    "             \"Carriage of Conditionally Permitted Shale Gas Extraction Waste Water in Bulk\",\n",
    "             \"Federal Motor Vehicle Safety Standards: Event Data Recorders\",\n",
    "             \"Documents Required for Travel Within the Western Hemisphere\"]\n",
    "doc_title2 = [\"FR 5597-P-02 Instituting Smoke- Free Public Housing\",\n",
    "             \"Head Start Program\",\n",
    "             \"Off-Road Vehicle Management: Cape Lookout National Seashore\",\n",
    "             \"Federal Acquisition Regulations: Fair Pay and Safe Workplaces; Second Extension of Time for Comments (FAR Case 2014-025)\",\n",
    "             \"Ability to Repay Standards under Truth in Lending Act (Regulation Z)\",\n",
    "             \"Schedule of Fees for Consular Services, Department of State and Overseas Embassies and Consulates\",\n",
    "             \"Carriage of Conditionally Permitted Shale Gas Extraction Waste Water in Bulk\",\n",
    "             \"Women-Owned Small Business Federal Contract Program\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "9\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "23\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(doc_list1)):\n",
    "    print(i)\n",
    "    info_dic = {}\n",
    "    doc_text = str(doc_list1[i]['text'][0])\n",
    "    info_dic[\"keywords\"],info_dic[\"sentences\"], info_dic[\"summary\"] = extract_summary(doc_text)\n",
    "    info_dic[\"doc_id\"], info_dic[\"doc_title\"] = doc_id1[i], doc_title1[i]\n",
    "    data.append(info_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(doc_list2)):\n",
    "    print(i)\n",
    "    info_dic = {}\n",
    "    doc_text = str(doc_list2[i]['text'][0])\n",
    "    info_dic[\"keywords\"],info_dic[\"sentences\"], info_dic[\"summary\"] = extract_summary(doc_text)\n",
    "    info_dic[\"doc_id\"], info_dic[\"doc_title\"] = doc_id2[i], doc_title2[i]\n",
    "    data.append(info_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "top_obj ={}\n",
    "top_obj[\"data\"] = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "with open('data/text_data.json', 'w') as outfile:\n",
    "    json.dump(top_obj, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
